{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMI \n",
    "Implementation by Gael Varoquaux:\n",
    "https://gist.github.com/GaelVaroquaux/ead9898bd3c973c40429\n",
    "\n",
 HEAD
    "**Problems:** \n",
    "-  why does entropy() return negative values for k=1? *[resolved?]*\n",
    "-  MI is negative for some data; see: https://www.researchgate.net/post/New_and_accurate_algorithm_of_mutual_information-any_thoughts"

    "**Question:** why does entropy() return negative values for k=1?"
 3a6eaaf686528ec63edbcb62e697e48c19b31d81
   ]
  },
  {
   "cell_type": "code",
 HEAD
   "execution_count": 1,

   "execution_count": 5,
 3a6eaaf686528ec63edbcb62e697e48c19b31d81
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import gamma,psi\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
 HEAD
   "execution_count": 2,

   "execution_count": 6,
 3a6eaaf686528ec63edbcb62e697e48c19b31d81
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_distances(X, k=1):\n",
    "    '''\n",
    "    X = array(N,M)\n",
    "    N = number of points\n",
    "    M = number of dimensions\n",
    "\n",
    "    returns the distance to the kth nearest neighbor for every point in X\n",
    "    '''\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(X)\n",
    "    d, _ = knn.kneighbors(X) # the first nearest neighbor is itself\n",
    "    return d[:, -1] # returns the distance to the kth nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
 HEAD
   "execution_count": 3,

   "execution_count": 7,
 3a6eaaf686528ec63edbcb62e697e48c19b31d81
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(X, k=1):\n",
    "    ''' Returns the entropy of the X.\n",
    "\n",
    "    Parameters\n",
    "    ====\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The data the entropy of which is computed\n",
    "\n",
    "    k : int, optional\n",
    "        number of nearest neighbors for density estimation\n",
    "\n",
    "    Notes\n",
    "    ======\n",
    "\n",
    "    Kozachenko, L. F. & Leonenko, N. N. 1987 Sample estimate of entropy\n",
    "    of a random vector. Probl. Inf. Transm. 23, 95-101.\n",
    "    See also: Evans, D. 2008 A computationally efficient estimator for\n",
    "    mutual information, Proc. R. Soc. A 464 (2093), 1203-1215.\n",
    "    and:\n",
    "    Kraskov A, Stogbauer H, Grassberger P. (2004). Estimating mutual\n",
    "    information. Phys Rev E 69(6 Pt 2):066138.\n",
    "    '''\n",
    "\n",
    "    # Distance to kth nearest neighbor\n",
    "    r = nearest_distances(X, k) # squared distances\n",
    "    n, d = X.shape\n",
    "    volume_unit_ball = (np.pi**(.5*d)) / gamma(.5*d + 1)\n",
    "    '''\n",
    "    F. Perez-Cruz, (2008). Estimation of Information Theoretic Measures\n",
    "    for Continuous Random Variables. Advances in Neural Information\n",
    "    Processing Systems 21 (NIPS). Vancouver (Canada), December.\n",
    "\n",
    "    return d*mean(log(r))+log(volume_unit_ball)+log(n-1)-log(k)\n",
    "    '''\n",
    "    return (d*np.mean(np.log(r + np.finfo(X.dtype).eps))\n",
    "            + np.log(volume_unit_ball) + psi(n) - psi(k))"
   ]
  },
  {
   "cell_type": "code",
 HEAD
   "execution_count": 4,

   "execution_count": 8,
 3a6eaaf686528ec63edbcb62e697e48c19b31d81
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2000)\n",
    "y = np.random.randn(2000)\n",
    "\n",
    "X = np.stack((x,y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
 HEAD
   "execution_count": 12,

   "execution_count": 9,
 3a6eaaf686528ec63edbcb62e697e48c19b31d81
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1,15):\n",
    "#     print('k = {:03} | entropy = {}'.format(i, entropy(X, k=i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(variables, k=1):\n",
    "    '''\n",
    "    Returns the mutual information between any number of variables.\n",
    "    Each variable is a matrix X = array(n_samples, n_features)\n",
    "    where\n",
    "      n = number of samples\n",
    "      dx,dy = number of dimensions\n",
    "    Optionally, the following keyword argument can be specified:\n",
    "      k = number of nearest neighbors for density estimation\n",
    "    Example: mutual_information((X, Y)), mutual_information((X, Y, Z), k=5)\n",
    "    '''\n",
    "    if len(variables) < 2:\n",
    "        raise AttributeError(\n",
    "                \"Mutual information must involve at least 2 variables\")\n",
    "    all_vars = np.hstack(variables)\n",
    "    \n",
    "    MI = sum([entropy(X, k=k) for X in variables]) - entropy(all_vars, k=k)\n",
    "    \n",
    "    return MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.randn(2000)\n",
    "y1 = np.random.randn(2000)\n",
    "\n",
    "Y = np.stack((x1,y1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
 HEAD
   "execution_count": 11,

   "execution_count": 12,
 3a6eaaf686528ec63edbcb62e697e48c19b31d81
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
 HEAD
      "k = 001 | MI = 8.871015284170227\n",
      "k = 002 | MI = -0.9589879733547741\n",
      "k = 003 | MI = -0.5301212317112522\n",
      "k = 004 | MI = -0.305492635891615\n",
      "k = 005 | MI = -0.19945599472979492\n",
      "k = 006 | MI = -0.1646682837055451\n",
      "k = 007 | MI = -0.14138287084539858\n",
      "k = 008 | MI = -0.11889146459683886\n",
      "k = 009 | MI = -0.09644198826533934\n",
      "k = 010 | MI = -0.07842616237635092\n",
      "k = 011 | MI = -0.07047437313787963\n",
      "k = 012 | MI = -0.06462134054967628\n",
      "k = 013 | MI = -0.05826576543482176\n",
      "k = 014 | MI = -0.049213897118057126\n",
      "k = 015 | MI = -0.048932311929985595\n",
      "k = 016 | MI = -0.042236489537290645\n",
      "k = 017 | MI = -0.03944933796959749\n",
      "k = 018 | MI = -0.03698488574345049\n",
      "k = 019 | MI = -0.0313917455689694\n"

      "k = 001 | entropy = -62.7647087887746\n",
      "k = 002 | entropy = 1.7735714988549924\n",
      "k = 003 | entropy = 2.28655503926748\n",
      "k = 004 | entropy = 2.472086096348222\n",
      "k = 005 | entropy = 2.552310862074207\n",
      "k = 006 | entropy = 2.596141065784604\n",
      "k = 007 | entropy = 2.6276482095951477\n",
      "k = 008 | entropy = 2.647119090720789\n",
      "k = 009 | entropy = 2.669327665916767\n",
      "k = 010 | entropy = 2.6844097243496194\n",
      "k = 011 | entropy = 2.6931338917128254\n",
      "k = 012 | entropy = 2.6956349031532247\n",
      "k = 013 | entropy = 2.704730824638077\n",
      "k = 014 | entropy = 2.707239216124299\n"
 3a6eaaf686528ec63edbcb62e697e48c19b31d81
     ]
    }
   ],
   "source": [
 HEAD
    "for i in range(1,20):\n",
    "    print('k = {:03} | MI = {}'.format(i, mutual_information((X,Y), k=i)))"

    "for i in range(1,15):\n",
    "    print('k = {:03} | entropy = {}'.format(i, entropy(X, k=i)))"
 3a6eaaf686528ec63edbcb62e697e48c19b31d81
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
