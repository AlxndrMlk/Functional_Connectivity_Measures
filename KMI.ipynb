{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMI \n",
    "Implementation by Gael Varoquaux:\n",
    "https://gist.github.com/GaelVaroquaux/ead9898bd3c973c40429\n",
    "\n",
    "**Question:** why does entropy() return negative values for k=1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import gamma,psi\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_distances(X, k=1):\n",
    "    '''\n",
    "    X = array(N,M)\n",
    "    N = number of points\n",
    "    M = number of dimensions\n",
    "\n",
    "    returns the distance to the kth nearest neighbor for every point in X\n",
    "    '''\n",
    "    knn = NearestNeighbors(n_neighbors=k)\n",
    "    knn.fit(X)\n",
    "    d, _ = knn.kneighbors(X) # the first nearest neighbor is itself\n",
    "    return d[:, -1] # returns the distance to the kth nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(X, k=1):\n",
    "    ''' Returns the entropy of the X.\n",
    "\n",
    "    Parameters\n",
    "    ===========\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The data the entropy of which is computed\n",
    "\n",
    "    k : int, optional\n",
    "        number of nearest neighbors for density estimation\n",
    "\n",
    "    Notes\n",
    "    ======\n",
    "\n",
    "    Kozachenko, L. F. & Leonenko, N. N. 1987 Sample estimate of entropy\n",
    "    of a random vector. Probl. Inf. Transm. 23, 95-101.\n",
    "    See also: Evans, D. 2008 A computationally efficient estimator for\n",
    "    mutual information, Proc. R. Soc. A 464 (2093), 1203-1215.\n",
    "    and:\n",
    "    Kraskov A, Stogbauer H, Grassberger P. (2004). Estimating mutual\n",
    "    information. Phys Rev E 69(6 Pt 2):066138.\n",
    "    '''\n",
    "\n",
    "    # Distance to kth nearest neighbor\n",
    "    r = nearest_distances(X, k) # squared distances\n",
    "    n, d = X.shape\n",
    "    volume_unit_ball = (np.pi**(.5*d)) / gamma(.5*d + 1)\n",
    "    '''\n",
    "    F. Perez-Cruz, (2008). Estimation of Information Theoretic Measures\n",
    "    for Continuous Random Variables. Advances in Neural Information\n",
    "    Processing Systems 21 (NIPS). Vancouver (Canada), December.\n",
    "\n",
    "    return d*mean(log(r))+log(volume_unit_ball)+log(n-1)-log(k)\n",
    "    '''\n",
    "    return (d*np.mean(np.log(r + np.finfo(X.dtype).eps))\n",
    "            + np.log(volume_unit_ball) + psi(n) - psi(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(2000)\n",
    "y = np.random.randn(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((x,y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 001 | entropy = -62.7647087887746\n",
      "k = 002 | entropy = 1.7735714988549924\n",
      "k = 003 | entropy = 2.28655503926748\n",
      "k = 004 | entropy = 2.472086096348222\n",
      "k = 005 | entropy = 2.552310862074207\n",
      "k = 006 | entropy = 2.596141065784604\n",
      "k = 007 | entropy = 2.6276482095951477\n",
      "k = 008 | entropy = 2.647119090720789\n",
      "k = 009 | entropy = 2.669327665916767\n",
      "k = 010 | entropy = 2.6844097243496194\n",
      "k = 011 | entropy = 2.6931338917128254\n",
      "k = 012 | entropy = 2.6956349031532247\n",
      "k = 013 | entropy = 2.704730824638077\n",
      "k = 014 | entropy = 2.707239216124299\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,15):\n",
    "    print('k = {:03} | entropy = {}'.format(i, entropy(X, k=i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
